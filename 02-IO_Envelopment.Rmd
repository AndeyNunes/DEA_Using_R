---
output:
  html_document: default
  pdf_document: default
---
# Input-Oriented Envelopment Model

Introduction
-----------------------------
Data envelopment analysis or DEA is a powerful tool for conducting studies of efficiency and has been used in thousands of publications since its inception in the 1970s.  

While tools exist for conducting the evaluations, it is important to understand how the tools work. Many DEA studies have been conducted and published by authors with only a superficial understanding of the technique.  This is equivalent to having a house built by carpenters that only (barely?) understand how to use a hammer.  The purpose of this document is to show how DEA works, what it means, and how to use R for getting started with DEA.  In order to keep things simple, this first step only looks at the input-oriented envelopment model with constant returns to scale.  We will consider other models soon.  

This chapter walks through how DEA works and then shows how to implement the model in R using two very different approaches.  Over the years, I have built DEA models in many languages and platforms:  Pascal, LINDO, LINGO, Excel Macros, Excel's VBA, GAMS, AMPL, and GLPK among others.  Each represents a different set of tradeoffs. 

Recently, my research group at PSU adopted the R platform. It too carries its own set of tradeoffs:

* Open-source so that people can dive as deep as they need and no risk of future vendor lock-in
* Freely available so it does not strain research budgets
* Extensive collection of numerical add-in packages (over 10000 at CRAN) to build upon
* Robust distribution of add-in packages could help improve distribution of our own contributions
* Multiple linear programming packages
* Widespread usage and acceptace in the research and analytics communities

Creating the Data
-----------------------------

Let's start by defining data.  DEA typically has inputs and outputs.  The input(s) are typically resources that are consumed in the production of output(s).  Inputs are referred to as "bads" in that higher levels at the same level of output is considered worse.  Similarly, holding everything else constant, an increase in any single output is laudable.  Examples of inputs might include capital, labor, or number of machines.

In contrast, outputs are the "good" items that are being produced by the actions of the producers.  Examples of outputs include automobiles produced, customers served, or 

Let's start by creating a simple dataset.  We'll assume that we have a small group of units.  Which of these units are the best?  Which are laggards?  

```{r}
x <- matrix(c(10,20,30,50),ncol=1,dimnames=list(LETTERS[1:4],"x"))

y <- matrix(c(75,100,300,400),ncol=1,dimnames=list(LETTERS[1:4],"y"))
```

For benchmarking, we want to know which ones are doing the best job.

Can you tell which units represent the best tradeoff between inputs and outputs?  None of the units are dominated by any of the other units.  

Graphical Analysis
-----------------------------

Let's start by doing a simple plot of the data.  For now, I'm going to make use of a function in Peter Bogetoft and Lars Otto's Benchmarking package which provides a very handy two-dimensional  plot in the format often used for showing production. 

```{r}
library(Benchmarking)

dea.plot(x, y, RTS="crs", ORIENTATION="in-out", txt=LETTERS[1:length(x)], 
         add=FALSE, wx=NULL, wy=NULL, TRANSPOSE=FALSE, fex=1, GRID=TRUE,
         RANGE=FALSE, param=NULL)
```

This chart clearly shows that unit _C_ has the best ratio of output (y) to input (x).  The diagonal line represents an efficiency frontier of best practices that could be achieved by scaling up or down unit _C_.  As the input is scaled up or down, it is assumed that the output of unit _C_ would be scaled up or down by the same value.  We will return to assumption in a later section but for now, think of this as saying that unit C cannot enjoy economies of scale by getting larger or diseconomies of scale by getting smaller so it is referred to as constant returns to scale or CRS.

Furthermore, we can graphically examine the technical efficiency of each of the other units.  I'm going to start with unit B since it is a little easier to visualize.  For now, let's think of the question, how much more or less input would _C_ require to produce as much output as _B_.  

To determine this efficiency score, simply draw a horizontal line from _B_ to the efficiency frontier on the left.  This point can be thought of a target for _B_ to be efficiency.  This point has the same output as _B_ but uses only half as much input.  The efficiency score can now be calculated as the ratio of the distance from the vertical axis to the target divided by the distance from the vertical axis to _B_.  This distance is simply 10/20 or 50%.  

Another question is how to construct the target for _B_'s evaluation.  It is simply made by scaling down _C_ to a third of its original size.  This results in a target that is composed of about 0.333 of _C_.  Also, it should be noted that it makes use of no part of _A_, _B_, or _D_.  

The same steps can be followed for analyzing units _A_, _C_, and _D_.  

The Linear Programs for DEA
-----------------------------

On the other hand, what if it allowed for blending of units.  There are a few assumptions that we could make.  Let's start by saying that we can compare any particular products by rescaling (up or down) any other product as well as any combination of units.  

We'll start by creating a mathematical framework. Can you find a combination of units that produces at least as much output using less input?  Let's define the proportion of input needed as $\theta$.  A value of $\theta=1$ then means no input reduction can be found in order to produce that unit's level of output.  The blend of other units is described by a vector $\lambda$.  Another way to denote this is $\lambda_j$ is the specific amount of a unit _j_ used in setting the target for for performance for unit _k_.  Similarly, $x_j$ is the amount of input used by unit _j_ and $y_j$ is the amount of output produced by unit _j_.  

This can be easily expanded to the multiple input and multiple output case by defining $x_{i,j}$ to  be the amount of the _i_'th input used by unit _j_ and $y_r,j$ to be the amount of the _r_'th output produced by unit _j_.  For simplicity, this example will focus on the one input and one output case rather than the _m_ input and _s_ output case but the R code explicitly allows for $m,s>1$.  To make the code more readable, I will use a slightly different convention _NX_ instead of _m_ to refer to the number of inputs (x's) and _NY_ to be the number of ouputs (y's) instead of _s_. Also, the normal mathematical convention is to use _n_ to denote the number of Decision Making Units (DMUs) so I will use _ND_ to indicate that in the R code.  

Let's connect these ideas together now using these mathematical building blocks.  The core idea of the envelopment model of a DMU _k_ can be thought of as to find a target constructed of a mix of the DMU's described by a vector $\lambda$ that uses no more input to achieve the same or more every output as DMU _k_.  The amount of the _i'th_ input used by the target is then $\sum_{j=1}^{n} x_{i,j}\lambda_j$.  By the same token, the amount of the _r'th_ output produced by the target is $\sum_{j=1}^{n} y_{r,j}\lambda_j$.  

This gives us two sets of constraints along with a restriction of non-negativity.  These are shown in the following relationships that must all be satisfied simultaneously.

$$
 \begin{split}
 \begin{aligned}
    \ & \sum_{j=1}^{n} x_{i,j}\lambda_j \leq x_{i,k} \forall \; i\\
                       & \sum_{j=1}^{n} y_{r,j}\lambda_j \geq  y_{r,k} \forall \; r\\
                       & \lambda_j \geq 0  \; \forall \; j
  \end{aligned}
  \end{split}
  (\#eq:ConstructingTargets)
$$

This is not yet a linear program because it is missing an objective function.  It defines what is an acceptable target of performance that is at least as good as DMU _k_ but does not try to find a _best_ target.

The two most common approaches to finding the best target are the input-oriented and output-oriented models. In the Output-oriented model, the first (input) constraint is satisfied while trying to _exceed_ the second constraint (output) by as much possible.  This focus on increasing the output is then called an _output orientation_.

In this chapter, we will focus on satisfying the second constraint while trying to improve upon the first by as much as possible.  In other words, we will satisfy the second (output) constraint but try to form a target that uses as little input as possible.

In fact, we will go one step further and say that we want to find the maximum input possible reduction in k's input or conversely, the minimum amount of the input that could be used by the target while still producing the same or more output.  We do this by adding a new variable, $/theta$, which is the radial reduction in the amount of DMU k's input.  We want to find how low we can drive this by _minimizing_ $/theta$.  This gives us the following linear program, often abbreviated as LP.  


$$
 \begin{split}
 \begin{aligned}
    \text{minimize  }   & \theta \\
    \text{subject to } & \sum_{j=1}^{n} x_{i,j}\lambda_j \leq \theta x_{i,k} \forall \; i\\
                       & \sum_{j=1}^{n} y_{r,j}\lambda_j \geq  y_{r,k} \forall \; r\\
                       & \lambda_j \geq 0  \; \forall \; j
  \end{aligned}
  \end{split}
  (\#eq:LPCCRIOE-NoSlack-Simple)
$$


The above LP was written using LaTeX embedded within this rmarkdown document.  Insights for this LP were found online, including: http://tex.stackexchange.com/questions/9625/stating-a-linear-program

Expressing the target on the left and the actual units's value and radial reduction on the right is conceptually straightforward to understand.   Unfortunately, optimization software typically requires collecting all the variables on the left and putting constants on the right hand side of the inequalities.  This is easily done.  

$$
\begin{split}
\begin{aligned}
    \text{minimize  }   & \theta \\
    \text{subject to } & \sum_{j=1}^{n} x_{i,j}\lambda_j - \theta x_{i,k} \leq 0 \forall \; i\\
                       & \sum_{j=1}^{n} y_{r,j}\lambda_j \geq  y_{r,k} \forall \; r\\
                       & \lambda_j \geq 0  \; \forall \; j
  \end{aligned}
 \end{split}
  (\#eq:LPCCRIOE-NoSlack)$$

## Creating the LP - The Algebraic Approach

There are two fundamentally different approaches to setting up linear programs for solving.  The first approach is to define data structures to pass vectors for the objective function coefficients and constraint right hand sides along with a matrix of data describing the constraints.  This requires careful setting up of the linear programs and is a big cognitive step away from the mathematical representation.  Another approach is to use algebraic modeling languages.  Standalone algebraic optimization modeling languages include LINGO, AMPL, GAMS, GMPL, and others.  Until recently, r did not have the ability to do algebraic modeling optimization but recently a few efforts have provided support for this. A new package, _ompr_, provides an algebraic perspective that matches closely to the summation representation of a linear program shown earlier.  Don't worry, if you want to see the data structure format approach, that is covered in the next chapter.

Let's define some data structures for holding our results.

```{r Declaring_Structures_of_Results_ompr}

  ND <- nrow(x); NX <- ncol(x); NY <- ncol(y); # Define data size

  xdata<-x[1:ND,] 
  dim(xdata)<-c(ND,NX) 
  ydata<-y[1:ND,]
  dim(ydata)<-c(ND,NY)
                 # Now we will create lists of names
  DMUnames <- list(c(LETTERS[1:ND]))               # DMU names: A, B, ...
  Xnames<- lapply(list(rep("X",NX)),paste0,1:NX)   # Input names: x1, ...
  Ynames<- lapply(list(rep("Y",NY)),paste0,1:NY)   # Output names: y1, ...
  Vnames<- lapply(list(rep("v",NX)),paste0,1:NX)   # Input weight names: v1, ...
  Unames<- lapply(list(rep("u",NY)),paste0,1:NY)   # Output weight names: u1, ...
  SXnames<- lapply(list(rep("sx",NX)),paste0,1:NX) # Input slack names: sx1, ...
  SYnames<- lapply(list(rep("sy",NY)),paste0,1:NY) # Output slack names: sy1, ...
  Lambdanames<- lapply(list(rep("L_",ND)),paste0,LETTERS[1:ND])

  results.efficiency <- matrix(rep(-1.0, ND), nrow=ND, ncol=1)
#  dimnames(results.efficiency)<-list(c(LETTERS[1:ND]),c("Eff"))
  dimnames(results.efficiency)<-c(DMUnames,"CCR-IO")  # Attach names
  
  results.lambda     <- matrix(rep(-1.0, ND^2), nrow=ND,ncol=ND)
  dimnames(results.lambda)<-c(DMUnames,Lambdanames)
  
  results.vweight    <- matrix(rep(-1.0, ND*NX), nrow=ND,ncol=NX)
  dimnames(results.vweight)<-c(DMUnames,Vnames)
  
  results.uweight    <- matrix(rep(-1.0, ND*NY), nrow=ND,ncol=NY) 
  dimnames(results.uweight)<-c(DMUnames,Unames)

  results.xslack     <- matrix(rep(-1.0, ND*NX), nrow=ND,ncol=NX) 
  dimnames(results.xslack)<-c(DMUnames,SXnames)

  results.yslack     <- matrix(rep(-1.0, ND*NY), nrow=ND,ncol=NY) 
  dimnames(results.yslack)<-c(DMUnames,SYnames)

```

We're going to use our data from earlier.  

```{r loading_packages, message=FALSE, warning=FALSE}
     # options: message=FALSE, warning=FALSE to turn off display

library(pander)           # Used for making nicely formatted tables
library(dplyr)            # For data structure manipulation
library(ROI)              # R Optimization Interface package
library(ROI.plugin.glpk)  # Connection to glpk as solver
library(ompr)             # Optimization Modeling using R
library(ompr.roi)         # Connective tissue

```
Now that we have loaded all of the packages that we use as building blocks, we can start constructing the model.

```{r first_ompr_model}

k<-1    # DMU to analyze.  Let's start with just one for now.

result <- MIPModel() %>%
  add_variable(vlambda[j], j = 1:ND, type = "continuous", lb = 0) %>%
  add_variable(vtheta, type = "continuous") %>%
  set_objective(vtheta, "min") %>%
  add_constraint(sum_expr(vlambda[j] * xdata[j,1], j = 1:ND) 
                 <= vtheta * xdata[k,1]) %>%
  add_constraint(sum_expr(vlambda[j] * ydata[j,1], j = 1:ND) 
                 >= ydata[k,1]) %>%
  solve_model(with_ROI(solver = "glpk")) 
#  filter(value > 0)

omprtheta <-  get_solution(result, vtheta) 
omprlambda <-  get_solution(result, vlambda[j])

ND <- 4 # Four Decision Making Units or DMUs
NX <- 1 # One input
NY <- 1 # One output

   # Only doing analysis for one unit at a time to start
results.efficiency <- matrix(rep(-1.0, 1), nrow=1, ncol=1)
results.lambda     <- matrix(rep(-1.0, ND), nrow=1,ncol=ND)

results.efficiency <- t(omprtheta)
colnames(results.efficiency) <- c("CCR-IO")
results.lambda <- t(omprlambda[3])
      # Takes the third column from the results and transposes results
      #    to be structured correctly for later viewing
colnames(results.lambda) <- c("L_A", "L_B", "L_C", "L_D")

pander(cbind(results.efficiency, results.lambda), 
       caption="Input-Oriented Envelopment Analysis for DMU A (CCR-IO)")
```

The above table follows a few conventions of labeling the efficiency score by the model used for the model, in this case the input-oriented variable returns to scale is labeled as CCR after Charnes, Cooper, and Rhodes.  Later we will cover other models including the variable returns to scale model, labeled BCC after Banker, Charnes, and Cooper.  Another convention is to use "L" in place of the Greek symbol $\lambda$ due to complications with using symbols in R matrix row or column names.  

Let's now extend it to handle multiple inputs, NX, and outputs, NY.  Of course this doesn't have any impact on our results just yet since we are still only using a single input and output but we now have the structure to accommodate the more general case.

```{r ompr_mult_IO, message=FALSE}

k<-1    # DMU to analyze.  Let's start with just one for now.

result <- MIPModel() %>%
  add_variable(vlambda[j], j = 1:ND, type = "continuous", lb = 0) %>%
  add_variable(vtheta, type = "continuous") %>%
  set_objective(vtheta, "min") %>%
  add_constraint(sum_expr(vlambda[j] * xdata[j,i], j = 1:ND) 
                 <= vtheta * xdata[k,i], i = 1:NX) %>%
  add_constraint(sum_expr(vlambda[j] * ydata[j,r], j = 1:ND) 
                 >= ydata[k,r], r = 1:NY) %>%
  solve_model(with_ROI(solver = "glpk")) 
#  filter(value > 0)

omprtheta <-  get_solution(result, vtheta) 
omprlambda <-  get_solution(result, vlambda[j])

results.efficiency <- t(omprtheta)
colnames(results.efficiency) <- c("CCR-IO")
results.lambda <- t(omprlambda[3])
      # Takes the third column from the results and transposes results
      #    to be structured correctly for later viewing
colnames(results.lambda) <- c("L_A", "L_B", "L_C", "L_D")

pander(cbind(results.efficiency, results.lambda), 
       caption="Repeated Results Allowing for Multiple Inputs and Outputs")

```

Now we should extend this to handle all four of the decision making units.  

```{r ompr_mult_DMUs, message=FALSE}

results.efficiency <- matrix(rep(-1.0, 1), nrow=ND, ncol=1)
results.lambda     <- matrix(rep(-1.0, ND), nrow=ND,ncol=ND)


for (k in 1:ND) {

  result <- MIPModel() %>%
    add_variable(vlambda[j], j = 1:ND, type = "continuous", lb = 0) %>%
    add_variable(vtheta, type = "continuous") %>%
    set_objective(vtheta, "min") %>%
    add_constraint(sum_expr(vlambda[j] * xdata[j,i], j = 1:ND) 
                   <= vtheta * xdata[k,i], i = 1:NX) %>%
    add_constraint(sum_expr(vlambda[j] * ydata[j,r], j = 1:ND) 
                   >= ydata[k,r], r = 1:NY) %>%
    solve_model(with_ROI(solver = "glpk")) 
#  filter(value > 0)
  
  print(c("DMU=",k,solver_status(result)))
#  print(get_solution(result, vlambda[1:(ND)]) )
    results.efficiency[k] <-  get_solution(result, vtheta)       
    results.lambda[k,] <- t(as.matrix(as.numeric(
              get_solution(result, vlambda[j])[,3] )))
       # Put lambda values in matrix
       # Comes out of ompr as a dataframe, use as.matrix 
       #    to convert from data frame to matrix
       #    grabs 3rd column to put into results matrix.  
       # First two columns of dataframe are for other info as.numeric 
       # forces the numbers to treated as numbers rather than text.
       # I'm sure there are better ways of handling this 
       # but I'll leave that for future work.

}
Lambdanames <- list("L_A", "L_B", "L_C", "L_D")
DMUnames <- list("A", "B", "C", "D")
dimnames(results.efficiency)<-list(DMUnames,"CCR-IO")
dimnames(results.lambda)<-list(DMUnames,Lambdanames)
pander (cbind(results.efficiency, results.lambda),
        caption="Input-Oriented Efficiency Results")

```

Success!  Each column of results.lambda reflect the way that a best target is made for that unit.

## Returns to Scale

Let's also add a constraint that will accommodate returns to scale.  All it needs to do is constrain the sum of the $\lambda$ variables equal to 1 enforces variables returns to scale or VRS.  The other very common returns to scale option is constant returns to scale or CRS.  For CRS, you can delete the constraint but it is helpful to maintain a consistent model size so we can make it a redundant constraint by constraining it to be greater than or equal to zero.  Since $\lambda$'s are by definition non-negative, so must the sum of $\lambda$'s and therefore the constraint is superfluous under CRS.

$$
\begin{split}
\begin{aligned}
    \text{minimize  }   & \theta \\
    \text{subject to } & \sum_{j=1}^{n} \lambda_j  = 1\\
                       & \sum_{j=1}^{n} x_{i,j}\lambda_j - \theta x_{i,k} + s^x_i = 0 \forall \; i\\
                       & \sum_{j=1}^{n} y_{r,j}\lambda_j - s^y =  y_{r,k} \forall \; r\\
                       & \lambda_j , s^x_i, s^y_r \geq 0  \; \forall i, \; r, \; j
  \end{aligned}
 \end{split}
  (\#eq:LPBCCIOE) 
$$

We can add another constraint to our previous model and solve it.  

```{r Adding_VRS}

RTS<-"VRS"
for (k in 1:ND) {
  
  result <- MIPModel() %>%
    add_variable(vlambda[j], j = 1:ND, type = "continuous", lb = 0) %>%
    add_variable(vtheta, type = "continuous") %>%
    set_objective(vtheta, "min") %>%
    add_constraint(sum_expr(vlambda[j] * xdata[j,i], j = 1:ND) 
                   <= vtheta * xdata[k,i], i = 1:NX) %>%
    add_constraint(sum_expr(vlambda[j] * ydata[j,r], j = 1:ND) 
                   >= ydata[k,r], r = 1:NY) 

    if (RTS=="VRS") {add_constraint(result, 
          sum_expr(vlambda[j], j = 1:ND) == 1) }  #Returns to Scale
#  filter(value > 0)

result <-   solve_model(result, with_ROI(solver = "glpk")) 
    
#  print(c("DMU=",k,solver_status(result)))  # Useful for diagnostics
#  print(get_solution(result, vlambda[1:(ND)]) )
    results.efficiency[k] <-  get_solution(result, vtheta)       
    results.lambda[k,] <- t(as.matrix(as.numeric(
              get_solution(result, vlambda[j])[,3] )))
       # Put lambda values in matrix
       # Comes out of ompr as a dataframe, use as.matrix 
       #    to convert from data frame to matrix
       #    grabs 3rd column to put into results matrix.  
       # First two columns of dataframe are for other info as.numeric 
       # forces the numbers to treated as numbers rather than text.
       # I'm sure there are better ways of handling this 
       # but I'll leave that for future work.

}
Lambdanames <- list("L_A", "L_B", "L_C", "L_D")
DMUnames <- list("A", "B", "C", "D")
dimnames(results.efficiency)<-list(DMUnames,"BCC-IO")
dimnames(results.lambda)<-list(DMUnames,Lambdanames)
pander (cbind(results.efficiency, results.lambda), 
        caption="Input-Oriented VRS Envelopment Results (BCC-IO)")

```

Notice that the efficiencies have generally increased or stayed the same.  Whereas earlier three out of four DMUs were inefficient, now three out of four are efficient.  Much more could be said about returns to scale.  One way of thinking of returns to scale is whether doubling the inputs should be expected to result in doubling the outputs that should be achieved.  Another way to think of it is whether it is fair to think of scaling up or down an efficient significantly to set a performance target for a much bigger or smaller unit.  For example, would it be _fair_ to compare a small convenience store such as 7-11 to a CostCo store scaled down by a factor of a 100?  

In addition to Constant returns to scale (CRS) and variable returns to scale (VRS), two other common approaches are increasing returns to scale (IRS) and decreasing returns to scale (DRS).  Technically, IRS is sometimes more formally referred to as non-decreasing returns to scale.  Similarly, DRS corresponds to non-increasing returns to scale. 

The actual returns to scale models are straightforward to implement.  

Returns to Scale | Envelopment Constraint
-----------------|---------------------
CRS    | No constraint needed
VRS    | $\sum_{j=1}^{n} \lambda_j  = 1$
IRS/NDRS | $\sum_{j=1}^{n} \lambda_j  >= 1$
DRS/NIRS | $\sum_{j=1}^{n} \lambda_j  <= 1$

Now, we will generalize this by allowing a parameter to set the returns to scale.

```{r Adding_RTS}

RTS<-"VRS"
for (k in 1:ND) {
  
  result <- MIPModel() %>%
    add_variable(vlambda[j], j = 1:ND, type = "continuous", lb = 0) %>%
    add_variable(vtheta, type = "continuous") %>%
    set_objective(vtheta, "min") %>%
    add_constraint(sum_expr(vlambda[j] * xdata[j,i], j = 1:ND) 
                   <= vtheta * xdata[k,i], i = 1:NX) %>%
    add_constraint(sum_expr(vlambda[j] * ydata[j,r], j = 1:ND) 
                   >= ydata[k,r], r = 1:NY) 

    if (RTS=="VRS") {add_constraint(result, sum_expr(vlambda[j], j = 1:ND) == 1) }  
    if (RTS=="IRS") {add_constraint(result, sum_expr(vlambda[j], j = 1:ND) >= 1) }  
    if (RTS=="DRS") {add_constraint(result, sum_expr(vlambda[j], j = 1:ND) <= 1) }  

result <-   solve_model(result, with_ROI(solver = "glpk")) 
    
#  print(c("DMU=",k,solver_status(result)))
#  print(get_solution(result, vlambda[1:(ND)]) )
    results.efficiency[k] <-  get_solution(result, vtheta)       
    results.lambda[,k] <- t(as.matrix(as.numeric(
              get_solution(result, vlambda[j])[,3] )))

}
Lambdanames <- list("L_A", "L_B", "L_C", "L_D")
DMUnames <- list("A", "B", "C", "D")
dimnames(results.efficiency)<-list(DMUnames,"BCC-IO")
dimnames(results.lambda)<-list(DMUnames,Lambdanames)
pander (cbind(results.efficiency, results.lambda), 
        caption="Input-Oriented BCC Model with Option of Alternate Returns to Scale")

```

## Multiple Inputs and Multiple Ouptuts

Using DEA for two-dimensional examples such as the one-input, one-output model is easy to draw and visualize but overkill.  

Let's create a richer dataset.  For this example, we will use the dataset from Kenneth Baker's third edition of _Optimization Modeling with Spreadsheets_, pages 175-178, Example 5.3 titled "Hope Valley Health Care Association." In this example, a health care organization wants to benchmark six nursing homes against each other.

```{r Multiple_inputs_outputs}
  XBaker1 <- matrix(c(150, 400, 320, 520, 350, 320, .2, 0.7, 1.2, 2.0, 1.2, 0.7),
                  ncol=2,dimnames=list(LETTERS[1:6],c("x1", "x2")))
  
```

This data is a little more complicated so let's make sure that we understand the definitions.  Look at the line for the inputs which we will call XBaker1.  The list of 12 data values has the structure defined as two columns for two separate inputs by use of 'ncol=2'.  This then means that there are 6 rows or DMUs in this dataset.  The six units are given capital letters for names starting with A.  The names of the inputs are hard coded as "x1" and"x2" to represent the _staff hours per day_ and the _supplies per day_ respectively.  

The two outputs of _reimbursed patient-days_ and _privately paid patient-days_ are named "y1" and "y2" but otherwise have the same structure as the inputs.  

```{r}
YBaker1 <- matrix(c(14000, 14000, 42000, 28000, 19000, 14000, 3500, 21000, 10500, 
                    42000, 25000, 15000),
                  ncol=2,dimnames=list(LETTERS[1:6],c("y1", "y2")))

ND <- nrow(XBaker1); NX <- ncol(XBaker1); NY <- ncol(YBaker1); # Define data size

```

Note that I'm naming the data sets based on their origin and then loading them into xdata and ydata for actual operation.

```{r Get_Baker_data_ready}
xdata      <-XBaker1[1:ND,]  # Call it xdata
dim(xdata) <-c(ND,NX)  # structure data correctly
ydata      <-YBaker1[1:ND,]
dim(ydata) <-c(ND,NY)

# Need to remember to restructure the results matrices.

results.efficiency <- matrix(rep(-1.0, ND), nrow=ND, ncol=1)
results.lambda     <- matrix(rep(-1.0, ND^2), nrow=ND,ncol=ND)
results.vweight    <- matrix(rep(-1.0, ND*NX), nrow=ND,ncol=NX) 
results.uweight    <- matrix(rep(-1.0, ND*NY), nrow=ND,ncol=NY) 
results.xslack     <- matrix(rep(-1.0, ND*NX), nrow=ND,ncol=NX) 
results.yslack     <- matrix(rep(-1.0, ND*NY), nrow=ND,ncol=NY) 

DMUnames <- list(c(LETTERS[1:ND]))
Xnames<- lapply(list(rep("X",NX)),paste0,1:NX)
Ynames<- lapply(list(rep("Y",NY)),paste0,1:NY)
Vnames<- lapply(list(rep("v",NX)),paste0,1:NX)
Unames<- lapply(list(rep("u",NY)),paste0,1:NY)
SXnames<- lapply(list(rep("sx",NX)),paste0,1:NX)
SYnames<- lapply(list(rep("sy",NY)),paste0,1:NY)
Lambdanames <- lapply(list(rep("L_",ND)),paste0,LETTERS[1:ND])
#Lambdanames<- lapply(list(rep("\u03BB",ND)),paste0,LETTERS[1:ND])
              # \u03BB is unicode for lambda symbol.
              # This would lambda symbol to be embedded               # in row or columns.
              # Unfortunately implementation of unicode
              # is not consistent enough in platform.
              # Instead, capital "L" is used for lambda symbol.
  
dimnames(xdata)<-c(DMUnames,Xnames)
dimnames(ydata)<-c(DMUnames,Ynames)
dimnames(results.efficiency)<-c(DMUnames,"CCR-IO")
dimnames(results.lambda)<-c(DMUnames,Lambdanames)
dimnames(results.vweight)<-c(DMUnames,Vnames)
dimnames(results.uweight)<-c(DMUnames,Unames)
dimnames(results.xslack)<-c(DMUnames,SXnames)
dimnames(results.yslack)<-c(DMUnames,SYnames)

```

We are now ready to do the analysis.  Note that Baker, does the analysis using the multiplier model and then from this reads out of the sensitivity report the lambda values.

```{r Baker_example}

RTS<-"CRS"
for (k in 1:ND) {
  
  result <- MIPModel() %>%
    add_variable(vlambda[j], j = 1:ND, type = "continuous", lb = 0) %>%
    add_variable(vtheta, type = "continuous") %>%
    set_objective(vtheta, "min") %>%
    add_constraint(sum_expr(vlambda[j] * xdata[j,i], j = 1:ND) 
                   <= vtheta * xdata[k,i], i = 1:NX) %>%
    add_constraint(sum_expr(vlambda[j] * ydata[j,r], j = 1:ND) 
                   >= ydata[k,r], r = 1:NY) 

    if (RTS=="VRS") {add_constraint(result, 
                 sum_expr(vlambda[j],j = 1:ND) == 1) }  #Returns to Scale
#  filter(value > 0)

result <-   solve_model(result, with_ROI(solver = "glpk")) 
    
    results.efficiency[k] <-  get_solution(result, vtheta)       
    results.lambda[k,] <- t(as.matrix(as.numeric(
                     get_solution(result, vlambda[j])[,3] )))
}
pander (cbind(results.efficiency, results.lambda),round=6, caption="Results from Baker's Example")

```

The efficiency scores match those reported in Baker, page 176, Figure 5.8.  The columns to the right of the efficiency sore are the lambdas.  The lambdas for facility 5 are given on page 177 and match the fifth column of our results.

In contrast, the second and third column warrant a more careful look.  Both of those columns report very small negative numbers.  These numbers are due to computational issues in the linear programming solvers and should be interpreted as zero.  

## Slack Maximization

Situations can arise where units may appear to be radially efficient but can still find opportunities to improve one or more inputs or outputs.  This is defined as weakly efficient.  

In order to accommodate this, we need to extend the simple radial model by adding variables to reflect nonradial slacks.  We do this by converting the model's input and output constraints from inequalities into equalities by explicitly defining slack variables.  

$$
\begin{split}
 \begin{aligned}
    \text{minimize  }   & \theta \\
    \text{subject to } & \sum_{j=1}^{n} x_{i,j}\lambda_j - \theta x_{i,k} + s^x_i = 0 \forall \; i\\
                       & \sum_{j=1}^{n} y_{r,j}\lambda_j - s^y_r =  y_{r,k} \forall \; r\\
                       & \lambda_j , s^x_i, s^y_r \geq 0  \; \forall \; j,i,r
  \end{aligned}
   \end{split}
  (\#eq:LPCCRIOE-Slacks)
$$

Simply formulating the model with slacks is insufficient.  We want to maximize these slacks after having found the best possible radial contraction (minimum value of $/theta$.)  This is done by adding a term summing the slacks to the objective function.  Note that this sum of slacks is multiplied by $\epsilon$ which is a  non-Archimedean infinitesimal.   The value of $\epsilon$ should be considered to be so small as to ensure that minimizing theta takes priority or maximizing the sum of slacks.  Note also that  maximizing the sum of slacks is done by substracting this sum of slacks.

$$
\begin{split}
 \begin{aligned}
    \text{minimize  }   & \theta - \epsilon ( \sum_{i} s^x_i + \sum_{r} s^y_r)\\
    \text{subject to } & \sum_{j=1}^{n} x_{i,j}\lambda_j - \theta x_{i,k} + s^x_i = 0 \forall \; i\\
                       & \sum_{j=1}^{n} y_{r,j}\lambda_j - s^y_r =  y_{r,k} \forall \; r\\
                       & \lambda_j , s^x_i, s^y_r \geq 0  \; \forall \; j,i,r
  \end{aligned}
 \end{split}
  (\#eq:LPCCRIOE-Two-Phase)
$$

A common mistake in implementing DEA is to use a finite approximation for $\epsilon$ such as $10^{-6}$.  Any finite value can cause distortions in $\theta$.  For example, an application comparing companies using revenue and expenses might have inputs and outputs on the order of millions or billions.  In this case, non-radial slacks could also be on the order of $10^{6}$.  Multiplying the two  results in a value similar in magnitude to the maximum possible efficiency score ($10^{-6}10^{6}=1$).

The proper way to implement slack maximization is to treat it as a preemptive goal programming problem. The primary goal is to minimize $\theta$ in a first phase linear program and the second goal, holding the level of $\theta$ fixed from the first phase is to then maximize the sum of the slacks.  

The first phase can take the form of any of our earlier linear programs without the $\epsilon$ term. The second phase is the following.

$$
\begin{split}
 \begin{aligned}
    \text{maximize  }  & \sum_{i}s^x_i + \sum_{r}s^y_r\\
    \text{subject to } & \sum_{j=1}^{n} \lambda_j  = 1\\
                       & \sum_{j=1}^{n} x_{i,j}\lambda_j - \theta x_{i,k} + s^x_i = 0 \; \forall \; i\\
                       & \sum_{j=1}^{n} y_{r,j}\lambda_j - s^y_r =  y_{r,k} \; \forall \; r\\
                       & \theta = \theta^*\\
                       & \lambda_j , s^x_i, s^y_r \geq 0  \; \forall \; j,i,r
  \end{aligned}
\end{split}
(\#eq:LPCCRIOE-Second-Phase)
$$

Implementing this algebraically is quite straightforward.

```{r Slack_max}

RTS<-"CRS"
for (k in 1:ND) {
  
  LPSlack <- MIPModel() %>%
    add_variable(vlambda[j], j = 1:ND, type = "continuous", lb = 0) %>%
    add_variable(vtheta, type = "continuous") %>%
    add_variable(xslack[i], i = 1:NX, type = "continuous", lb=0) %>%
    add_variable(yslack[r], r = 1:NY, type = "continuous", lb=0) %>%
    
    set_objective(vtheta, "min") %>%
    
    add_constraint(sum_expr(vlambda[j] * xdata[j,i]+xslack[i], j = 1:ND) 
                   - vtheta * xdata[k,i]==0, i = 1:NX) %>%
    
    add_constraint(sum_expr(vlambda[j] * ydata[j,r]-yslack[r], j = 1:ND) 
                    ==ydata[k,r], r = 1:NY) 

    if (RTS=="VRS") {add_constraint(LPSlack, sum_expr(vlambda[j],
                                j = 1:ND) == 1) }  #Returns to Scale

    result <-   solve_model(LPSlack, with_ROI(solver = "glpk")) 

    phase1obj <-  get_solution(result, vtheta)    

    add_constraint(LPSlack, vtheta==phase1obj)   
                     # Passing result from phase 1 to phase 2
    set_objective(LPSlack, sum_expr(
              xslack[i], i=1:NX)+sum_expr(yslack[r], r=1:NY), "max")
              # Modify the objective function for phase 2
    result <-   solve_model(LPSlack, with_ROI(solver = "glpk")) 
  
    results.efficiency[k] <-  get_solution(result, vtheta)       
    results.lambda[k,] <- t(as.matrix(
                 as.numeric(get_solution(result, vlambda[j])[,3] )))
}

rownames(results.efficiency)<-c("A", "B", "C", "D", "E", "F")
#print (results.efficiency)
pander(cbind(results.efficiency,xdata,ydata,results.lambda), round=5, caption="Input-oriented Efficiency Results")

```

The rightmost columns are the lambda values.  

##To-Do for Chapter

* Enrich example of slacks
* Add graphical figure showing slack maximization
* Generalize column name setting for ND where appropriate.

